{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pyodbc\n",
    "import arcpy\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import glob\n",
    "import shutil\n",
    "# This is using Andy's Census API KEy\n",
    "census_api_key = '9a73d08c296b844e58f1c70bd19c831826da5cbf'\n",
    "\n",
    "# Need to define datatypes so that FIPS code doesn't get cast as int and drop leading 0s\n",
    "dtypes = {\n",
    "    'YEAR' : str,\n",
    "    'STATE': str,\n",
    "    'GEOGRAPHY': str,\n",
    "    'GEOID': str,\n",
    "    'TRPAID':str,\n",
    "    'NEIGHBORHOOD': str\n",
    "}\n",
    "\n",
    "#Manually defined list of census tracts that are within the basin\n",
    " \n",
    "service_url = 'https://maps.trpa.org/server/rest/services/Demographics/MapServer/27'\n",
    "\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "tahoe_geometry_fields = ['YEAR', 'STATE', 'GEOGRAPHY', 'GEOID', 'TRPAID', 'NEIGHBORHOOD']\n",
    "query_result = feature_layer.query(out_fields=\",\".join(tahoe_geometry_fields))\n",
    "# Convert the query result to a list of dictionaries\n",
    "feature_list = query_result.features\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "tahoe_geometry = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function that is used to concatenate census data return\n",
    "def create_or_append_df(df, summary_df):\n",
    "    if df.empty:\n",
    "        df = summary_df.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, summary_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "#This gets the result of the get request and does some data wrangling to make it fit our structure\n",
    "def get_request_census(request_url, sample_level, geo_name):\n",
    "    response = requests.get(request_url)\n",
    "    #print(response.status_code)\n",
    "    df = pd.DataFrame(response.json())\n",
    "    #The json returns column names in the first row\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df['sample_level']=sample_level\n",
    "    df['Geo_Name']=geo_name\n",
    "    #Might as well add counties and states at this stage\n",
    "    return df\n",
    "\n",
    "def get_jobs_data(year, census_geom_year, variable, variablename, census_api_key, tahoe_geometry, variable_category):\n",
    "    base_url = 'https://api.census.gov/data/'\n",
    "    df_total=pd.DataFrame()\n",
    "    #Formatting to match html get request\n",
    "    #get the zipcodes for inclusion from tahoe_geometry\n",
    "    zipcodes = tahoe_geometry['TRPAID'].loc[(tahoe_geometry['YEAR']==census_geom_year)&(tahoe_geometry['GEOGRAPHY']=='ZIP CODE')].str[:-4]\n",
    "    print(zipcodes)\n",
    "    \n",
    "    for zipcode in zipcodes:\n",
    "        #print(f'{base_url}/{year}/cbp?get={variable}&for={geometry_return}:*&in=state:{state}%20county:{county}{geometry_level}&key={census_api_key}')\n",
    "        request_url = f'{base_url}{year}/cbp?get=GEO_ID,{variable}&for=zip%20code:{zipcode}&key={census_api_key}'\n",
    "        print (request_url)\n",
    "        response = requests.get(request_url)\n",
    "        \n",
    "        df = pd.DataFrame(response.json())\n",
    "        #The json returns column names in the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]\n",
    "        #Might as well add counties and states at this stage\n",
    "        if df_total.empty:\n",
    "            df_total=df\n",
    "        else:\n",
    "            df_total=pd.concat([df_total, df])\n",
    "\n",
    "    #Figure out exactly what variable we want here\n",
    "    #Add something here to handle margin of error\n",
    "    df_total['variable_code']=variable\n",
    "    df_total['variable_name']=variablename\n",
    "    df_total['variable_category']= variable_category\n",
    "    df_total['year_sample']=year\n",
    "    df_total['sample_level']='ZIP CODE'\n",
    "    df_total['dataset']= 'cbp'\n",
    "    df_total['census_geom_year'] = census_geom_year\n",
    "    df_total['GEO_ID'] = df_total['GEO_ID'].str.split('US').str[1]\n",
    "    df_total['TRPAID'] = df_total['GEO_ID']+df_total['census_geom_year'].astype(str)\n",
    "    df_total.columns.values[1] = 'value'\n",
    "    df_total['value'] = df_total['value'].astype(float)\n",
    "    df_total.insert(2, 'MarginOfError', np.NaN)\n",
    "    return df_total\n",
    "\n",
    "\n",
    "def get_variable_data(year, dataset, geometry_return, variable, variablename, census_api_key, census_geom_year, tahoe_geometry, variable_category):\n",
    "    #Returns all data for a given dataset for Washoe, El Dorado, Carson City, Douglas, Placer Counties\n",
    "    #Need to make five seperate api calls because of the geometry structure\n",
    "    county_states ={\n",
    "        '06': ['017','061'],\n",
    "        '32': ['005', '031']\n",
    "    }\n",
    "    base_url = 'https://api.census.gov/data'\n",
    "    df_total=pd.DataFrame()\n",
    "    #Formatting to match html get request\n",
    "    geometry_return=geometry_return.replace(\" \", \"%20\")\n",
    "    #This adds tract level to make block groups or blocks get request valid\n",
    "    if geometry_return == 'tract':\n",
    "        geometry_level = ''\n",
    "    else:\n",
    "        geometry_level='%20tract:*'\n",
    "    if 'acs/acs5' in dataset:\n",
    "        variable= variable +'E,'+variable + 'M'\n",
    "\n",
    "    \n",
    "    for state in county_states:\n",
    "        for county in county_states[state]:\n",
    "            #print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for={geometry_return}:*&in=state:{state}%20county:{county}{geometry_level}&key={census_api_key}')\n",
    "            request_url = f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for={geometry_return}:*&in=state:{state}%20county:{county}{geometry_level}&key={census_api_key}'\n",
    "            response = requests.get(request_url)\n",
    "            \n",
    "            df = pd.DataFrame(response.json())\n",
    "            #The json returns column names in the first row\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "            #Might as well add counties and states at this stage\n",
    "            if df_total.empty:\n",
    "                df_total=df\n",
    "            else:\n",
    "                df_total=pd.concat([df_total, df])\n",
    "    #Figure out exactly what variable we want here\n",
    "    #Add something here to handle margin of error\n",
    "    df_total['variable_code']=variable\n",
    "    df_total['variable_name']=variablename\n",
    "    df_total['variable_category']= variable_category\n",
    "    df_total['year_sample']=year\n",
    "    df_total['sample_level']=geometry_return.replace(\"%20\", \" \")\n",
    "    df_total['dataset']= dataset\n",
    "    df_total['census_geom_year'] = census_geom_year\n",
    "    df_total['GEO_ID'] = df_total['GEO_ID'].str.split('US').str[1]\n",
    "    df_total['TRPAID'] = df_total['GEO_ID']+df_total['census_geom_year'].astype(str)\n",
    "    df_total.columns.values[1] = 'value'\n",
    "    df_total['value'] = df_total['value'].astype(float)\n",
    "    if 'acs/acs5' in dataset:\n",
    "        df_total.columns.values[2]='MarginOfError'\n",
    "        df_total['variable_code'] = df_total['variable_code'].str.split(',').str[0]\n",
    "    else:\n",
    "        df_total.insert(2, 'MarginOfError', np.NaN)\n",
    "    if geometry_return == 'tract':\n",
    "        tract_col_loc = df_total.columns.get_loc('tract')\n",
    "        df_total.insert(tract_col_loc, 'block group', np.NaN)\n",
    "\n",
    "    #filter to just the tahoe parcels\n",
    "    df_total = df_total[df_total['TRPAID'].isin(tahoe_geometry['TRPAID'])]\n",
    "    df_total =  pd.merge(df_total, tahoe_geometry[['TRPAID', 'NEIGHBORHOOD']], on='TRPAID', how= 'left')\n",
    "    \n",
    "    return df_total\n",
    "\n",
    "def get_non_tahoe_data(year,dataset, variable, variablename, census_api_key, census_geom_year, variable_category):\n",
    "    base_url = 'https://api.census.gov/data'\n",
    "    df_total=pd.DataFrame()\n",
    "    county_states ={\n",
    "        '06': ['017','061'],\n",
    "        '32': ['005', '031', '510']\n",
    "    }\n",
    "    state_names={\n",
    "        '06':'CA',\n",
    "        '32':'NV'\n",
    "    }\n",
    "    county_names={\n",
    "        '017':'El Dorado County',\n",
    "        '061':'Placer County',\n",
    "        '005':'Douglas County',\n",
    "        '031':'Washoe County',\n",
    "        '510':'Carson City County'\n",
    "    }\n",
    "    #Need to update this so that it handles the different years - are 2010 and 2020 the same?\n",
    "    urban_centers = {\n",
    "        'Reno-Sparks MSA':'39900',\n",
    "        'Sacramento MSA': '40900',   \n",
    "    }\n",
    "    combined_metro_areas={\n",
    "        'Sanfranciso CMSA': '488'\n",
    "    }\n",
    "    urban_centers_2000 = {\n",
    "        'Reno-Sparks MSA':'6720',\n",
    "        'Sacramento MSA': '6922',   \n",
    "    }\n",
    "    combined_metro_areas_2000={\n",
    "        'Sanfranciso CMSA': '7362'\n",
    "    }\n",
    "    if year!=\"2000\":\n",
    "        for urban_center in urban_centers:\n",
    "            urban_center_code = urban_centers[urban_center]\n",
    "            print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=metropolitan%20statistical%20area/micropolitan%20statistical%20area:{urban_center_code}&key={census_api_key}')\n",
    "            request_url = f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=metropolitan%20statistical%20area/micropolitan%20statistical%20area:{urban_center_code}&key={census_api_key}'            \n",
    "            df = get_request_census(request_url,'MSA', urban_center)\n",
    "            df_total = create_or_append_df(df_total, df)    \n",
    "        for cma in combined_metro_areas:\n",
    "            cma_code = combined_metro_areas[cma]\n",
    "            print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=combined%20statistical%20area:{cma_code}&key={census_api_key}')\n",
    "            request_url = f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=combined%20statistical%20area:{cma_code}&key={census_api_key}'\n",
    "            df = get_request_census(request_url, 'MSA', cma)\n",
    "            df_total = create_or_append_df(df_total, df)\n",
    "    # for urban_center in urban_centers_2000:\n",
    "    #         urban_center_code = urban_centers_2000[urban_center]\n",
    "    #         statistical_region_url = f'metropolitan%20statistical%20area/micropolitan%20statistical%20area'\n",
    "    #         print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for={statistical_region_url}:{urban_center_code}&key={census_api_key}')\n",
    "    #         request_url= f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for={statistical_region_url}:{urban_center_code}&key={census_api_key}'\n",
    "    #         df = get_request_census(request_url,'MSA',urban_center)\n",
    "    #         df_total = create_or_append_df(df_total,df)\n",
    "        \n",
    "    for state in county_states:\n",
    "        for county in county_states[state]:\n",
    "            #https://api.census.gov/data/2010/dec/sf1?get=GEO_ID,P001001&for=county:017&in=state:06&key=9a73d08c296b844e58f1c70bd19c831826da5cbf\n",
    "            print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=county:{county}&in=state:{state}&key={census_api_key}')\n",
    "            request_url = f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=county:{county}&in=state:{state}&key={census_api_key}'\n",
    "            countyname = county_names[county]\n",
    "            df = get_request_census(request_url, 'County', countyname)\n",
    "            df_total = create_or_append_df(df_total, df)\n",
    "    for state in county_states:\n",
    "        #https://api.census.gov/data/2010/dec/sf1?get=GEO_ID,P001001&for=county:017&in=state:06&key=9a73d08c296b844e58f1c70bd19c831826da5cbf\n",
    "        print(f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=state:{state}&key={census_api_key}')\n",
    "        request_url = f'{base_url}/{year}/{dataset}?get=GEO_ID,{variable}&for=state:{state}&key={census_api_key}'\n",
    "        geoname = state_names[state]\n",
    "        df = get_request_census(request_url,'State', geoname)\n",
    "        df_total = create_or_append_df(df_total, df)\n",
    "        \n",
    "    #Figure out exactly what variable we want here\n",
    "    df_total['variable_code']=variable\n",
    "    df_total['variable_name']=variablename\n",
    "    df_total['variable_category']= variable_category\n",
    "    df_total['year_sample']=year\n",
    "    df_total['dataset']= dataset\n",
    "    df_total['census_geom_year'] = census_geom_year\n",
    "    df_total['GEO_ID'] = df_total['GEO_ID'].str.split('US').str[1]\n",
    "    df_total['GEO_CODE'] = df_total['GEO_ID']+df_total['census_geom_year'].astype(str)\n",
    "    df_total.columns.values[1] = 'value'\n",
    "    return df_total\n",
    "\n",
    "def census_download_wrapper (variable_file):\n",
    "    dtypes = {\n",
    "    'Variable' : str,\n",
    "    'Code': str,\n",
    "    'Category': str,\n",
    "    'Datasource': str,\n",
    "    'CodeNumber':str,\n",
    "    'Year':str,\n",
    "    'census_geom_year':str,\n",
    "    'GeometryLevel':str\n",
    "    }\n",
    "\n",
    "\n",
    "    variables = pd.read_csv(variable_file,dtype=dtypes)\n",
    "\n",
    "    #Loop through this?\n",
    "    df_values=pd.DataFrame()\n",
    "    for index, row in variables.iterrows():\n",
    "        #print(index)\n",
    "        \n",
    "        df = get_variable_data(row['Year'], row['Datasource Name'],row['GeometryLevel'],row['CodeNumber'],row['Variable'], census_api_key, row['census_geom_year'], tahoe_geometry, row['Category'])\n",
    "        \n",
    "        df_values = create_or_append_df(df_values, df)\n",
    "    return df_values\n",
    "\n",
    "def census_download_wrapper_non_tahoe(variable_file):\n",
    "    dtypes = {\n",
    "    'Variable' : str,\n",
    "    'Code': str,\n",
    "    'Category': str,\n",
    "    'Datasource': str,\n",
    "    'CodeNumber':str,\n",
    "    'Year':str,\n",
    "    'census_geom_year':str,\n",
    "    'GeometryLevel':str\n",
    "    }\n",
    "    variables = pd.read_csv(variable_file,dtype=dtypes)\n",
    "\n",
    "    #Loop through this?\n",
    "    df_values=pd.DataFrame()\n",
    "    for index, row in variables.iterrows():\n",
    "        print(index)\n",
    "        df = get_non_tahoe_data(row['Year'], row['Datasource Name'], row['CodeNumber'], row['Variable'], census_api_key, row['census_geom_year'], row['Category'])\n",
    "        df_values = create_or_append_df(df_values, df)\n",
    "    return df_values\n",
    "\n",
    "def load_variable_multiple_year(year_range, dataset, geometry_return, variable, variablename, census_api_key, tahoe_geometry, variable_category):\n",
    "    df=pd.DataFrame()\n",
    "    df_return=pd.DataFrame()\n",
    "    #year_range = [str(num) for num in range(year_start, year_end+1)]\n",
    "    for year in year_range:\n",
    "        if year in ['2020', '2021', '2022']:\n",
    "            census_geom_year = '2020'\n",
    "        else:\n",
    "            census_geom_year = '2010'\n",
    "        df = get_variable_data(year,dataset,geometry_return,variable,variablename,census_api_key, census_geom_year, tahoe_geometry, variable_category)\n",
    "        print(len(df))\n",
    "        df_return = create_or_append_df(df_return, df)\n",
    "        \n",
    "    return df_return\n",
    "#Import all csvs from a folder and merge them into a single dataframe\n",
    "def import_csvs(folder_path):\n",
    "    import os\n",
    "    import glob\n",
    "    os.chdir(folder_path)\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    return combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv(file_path, output_folder):\n",
    "    # Read the full CSV into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the base file name without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Define the number of rows per split file (not counting the header)\n",
    "    rows_per_file = 2\n",
    "    \n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Loop to create each smaller CSV\n",
    "    for i in range(0, len(df), rows_per_file):\n",
    "        # Select the current chunk of rows\n",
    "        df_chunk = df.iloc[i:i + rows_per_file]\n",
    "        \n",
    "        # Create the output file path with the base name and part number\n",
    "        output_file = os.path.join(output_folder, f\"{base_name}_part_{i // rows_per_file + 1}.csv\")\n",
    "        \n",
    "        # Write the chunk to a new CSV, including the header\n",
    "        df_chunk.to_csv(output_file, index=False)\n",
    "        print(f\"Created {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "#split_csv('your_input_file.csv', 'output_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_1.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_2.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_3.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_4.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_5.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_6.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_7.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_8.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_9.csv\n",
      "Created Split_Lookup_Lists\\dec_dhc_variables_age_part_10.csv\n"
     ]
    }
   ],
   "source": [
    "split_csv(file_path=r'C:\\Users\\amcclary\\Documents\\GitHub\\Housing\\Scripts\\Variable_Lists\\dec_dhc_variables_age.csv',output_folder='Split_Lookup_Lists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_nonhispanic = census_download_wrapper(r'C:\\Users\\amcclary\\Documents\\GitHub\\Housing\\Scripts\\Variable_Lists\\acs_variables_additional.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_nonhispanic.to_csv('non_hispanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'Split_Lookup_Lists'\n",
    "output_folder = 'Dowloaded_Data'\n",
    "processed_folder = 'Completed_Lookup_Lists'\n",
    "# Ensure the output and processed folders exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "for file_path in glob.glob(os.path.join(input_folder, '*.csv')):\n",
    "        # Read the CSV into a DataFrame\n",
    "        #df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Process the DataFrame with the provided function\n",
    "        df = census_download_wrapper(file_path)\n",
    "        \n",
    "        # Save the processed DataFrame to the output folder with the same file name\n",
    "        output_file_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        #print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        # Move the original file to the processed folder\n",
    "        processed_file_path = os.path.join(processed_folder, os.path.basename(file_path))\n",
    "        shutil.move(file_path, processed_file_path)\n",
    "        #print(f\"Moved original file to: {processed_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all csvs from a folder and merge them into a single dataframe\n",
    "output_folder = 'Dowloaded_Data\\Age_Data'\n",
    "def import_csvs(folder_path):\n",
    "    import os\n",
    "    import glob\n",
    "    os.chdir(folder_path)\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "    return combined_csv\n",
    "combined_csv = import_csvs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a leading 0 to the TRPAID column if the length is 15\n",
    "def add_leading_zero_based_on_condition(row):\n",
    "    if row['sample_level'] == 'tract' and len(row['TRPAID']) == 14:\n",
    "        return row['TRPAID'].zfill(15)\n",
    "    elif row['sample_level'] == 'block group' and len(row['TRPAID']) == 15:\n",
    "        return row['TRPAID'].zfill(16)\n",
    "    else:\n",
    "        return row['TRPAID']\n",
    "\n",
    "combined_csv['TRPAID'] = combined_csv['TRPAID'].astype(str)\n",
    "combined_csv['TRPAID'] = combined_csv.apply(add_leading_zero_based_on_condition, axis=1)\n",
    "combined_csv\n",
    "combined_csv['state'] = combined_csv['state'].astype(str).str.zfill(2)\n",
    "combined_csv['county'] = combined_csv['county'].astype(str).str.zfill(3)\n",
    "# Drop rows with a missing value\n",
    "combined_csv = combined_csv.dropna(subset=['value'])\n",
    "combined_csv.to_csv('combined_csv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
