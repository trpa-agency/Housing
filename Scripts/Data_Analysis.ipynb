{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "\n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get data for parcels, join census assign to geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_conn('sde')\n",
    "with engine.begin() as conn:\n",
    "    df_census = pd.read_sql(\"SELECT * FROM SDE.Census_Demographics\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_census_data(csv_path):\n",
    "    variables_demographics = pd.read_csv(csv_path)\n",
    "    # filter df_census to only include variables in the variables_demographics list joined on sample_year, sample_level and variable_name\n",
    "    df_census_demographics = df_census.merge(variables_demographics, how='inner', left_on=['year_sample', 'sample_level', 'variable_code'], right_on=['year', 'geography', 'variable_code'])\n",
    "    # group df_census_demographics by sample_year, tract, and variable_name and sum the values\n",
    "    df_census_demographics_grouped = df_census_demographics.groupby(['year_sample', 'tract','state', 'Description',\n",
    "            'county', 'variable_name', 'variable_code', 'census_geom_year_x'])['value'].sum().reset_index()\n",
    "    df_census_demographics_grouped['year_sample'] = df_census_demographics_grouped['year_sample'].astype(int).astype(str)\n",
    "    df_census_demographics_grouped['census_geom_year_x'] = df_census_demographics_grouped['census_geom_year_x'].astype(int).astype(str)\n",
    "    df_census_demographics_grouped['trpa_id'] = df_census_demographics_grouped['state'] + df_census_demographics_grouped['county'] + df_census_demographics_grouped['tract']+df_census_demographics_grouped['census_geom_year_x']\n",
    "    #Rename census_geom_year_x to census_geom_year\n",
    "    df_census_demographics_grouped.rename(columns={'census_geom_year_x': 'census_geom_year'}, inplace=True)\n",
    "    return df_census_demographics_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_variables = pd.read_csv(r'C:\\Users\\amcclary\\Documents\\GitHub\\Housing\\Scripts\\Lookup_Lists\\chas_variables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahoe_census_tracts = pd.read_csv(r'C:\\Users\\amcclary\\Documents\\GitHub\\Housing\\Scripts\\Lookup_Lists\\tahoe_census_tracts.csv')\n",
    "tahoe_census_tracts['geoid'] = tahoe_census_tracts['GEO_ID'].astype(str).str.zfill(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open(r'C:\\Users\\amcclary\\Downloads\\2017thru2021-140-csv\\140\\Table1.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result)  # Outputs the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_data_1 = pd.read_csv(r'C:\\Users\\amcclary\\Downloads\\2017thru2021-140-csv\\140\\Table1.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_data_1['state'] = chas_data_1['st'].astype(str).str.zfill(2)\n",
    "chas_data_1['county'] = chas_data_1['cnty'].astype(str).str.zfill(3)\n",
    "# Drop rows with a missing value\n",
    "# drop everything in the geoid column to the left of \"US\"\n",
    "chas_data_1['geoid'] = chas_data_1['geoid'].str.split('US').str[1]\n",
    "chas_data_tahoe = chas_data_1[chas_data_1['geoid'].isin(tahoe_census_tracts['geoid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically just need to clean geoid to match our list of geoids and then filter it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_states ={\n",
    "        '06': ['017','061'],\n",
    "        '32': ['005', '031']}\n",
    "# Filter chas_data_1 to only include data for the specified counties\n",
    "chas_data_local = pd.DataFrame()\n",
    "for state, counties in county_states.items():\n",
    "    if chas_data_local.empty:\n",
    "        chas_data_local = chas_data_1[(chas_data_1['state'] == state) & (chas_data_1['county'].isin(counties))]\n",
    "    else:\n",
    "        chas_data_local = pd.concat([chas_data_local, chas_data_1[(chas_data_1['state'] == state) & (chas_data_1['county'].isin(counties))]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all columns that start with T1\n",
    "t1_columns = [col for col in chas_data_tahoe.columns if col.startswith('T')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_data_melted = chas_data_tahoe.melt(id_vars=['state', 'county', 'geoid'], value_vars=t1_columns, var_name='variable_code', value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_data_1 = pd.read_csv(r'C:\\Users\\amcclary\\Downloads\\2017thru2021-140-csv\\140\\Table1.csv', encoding='ISO-8859-1')\n",
    "chas_data_1['state'] = chas_data_1['st'].astype(str).str.zfill(2)\n",
    "chas_data_1['county'] = chas_data_1['cnty'].astype(str).str.zfill(3)\n",
    "chas_data_1['geoid'] = chas_data_1['geoid'].str.split('US').str[1]\n",
    "chas_data_tahoe = chas_data_1[chas_data_1['geoid'].isin(tahoe_census_tracts['geoid'])]\n",
    "t1_columns = [col for col in chas_data_tahoe.columns if col.startswith('T')]\n",
    "chas_data_melted = chas_data_tahoe.melt(id_vars=['state', 'county', 'geoid'], value_vars=t1_columns, var_name='variable_code', value_name='value')\n",
    "chas_variables.rename(columns={'Column/Variable Name': 'variable_code'}, inplace=True)\n",
    "chas_data_local_merged = chas_data_melted.merge(chas_variables, how='inner', on='variable_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to process each CSV\n",
    "def process_chas_data(csv_path, tahoe_census_tracts, chas_variables):\n",
    "    # Load CSV data\n",
    "    chas_data_1 = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
    "\n",
    "    # Format state, county, and geoid columns\n",
    "    chas_data_1['state'] = chas_data_1['st'].astype(str).str.zfill(2)\n",
    "    chas_data_1['county'] = chas_data_1['cnty'].astype(str).str.zfill(3)\n",
    "    chas_data_1['geoid'] = chas_data_1['geoid'].str.split('US').str[1]\n",
    "\n",
    "    # Filter for Tahoe Basin data (assuming tahoe_census_tracts is a DataFrame with 'geoid' column)\n",
    "    chas_data_tahoe = chas_data_1[chas_data_1['geoid'].isin(tahoe_census_tracts['geoid'].tolist())]\n",
    "\n",
    "    # Select columns that start with 'T'\n",
    "    t1_columns = [col for col in chas_data_tahoe.columns if col.startswith('T')]\n",
    "        # Rename columns in chas_variables\n",
    "    chas_data_tahoe.rename(columns={'Column/Variable Name': 'variable_code'}, inplace=True)\n",
    "    # Melt the data (long format)\n",
    "    chas_data_melted = chas_data_tahoe.melt(id_vars=['state', 'county', 'geoid'], value_vars=t1_columns, var_name='variable_code', value_name='value')\n",
    "\n",
    "\n",
    "\n",
    "    # Merge the melted data with the variable names\n",
    "    chas_data_local_merged = chas_data_melted.merge(chas_variables, how='inner', on='variable_code')\n",
    "\n",
    "    return chas_data_local_merged\n",
    "\n",
    "\n",
    "# Function to process all CSV files in a directory and combine into a single DataFrame\n",
    "def process_all_csv_in_directory(directory_path, tahoe_census_tracts, chas_variables):\n",
    "    all_data = []  # List to store individual DataFrames\n",
    "\n",
    "    # Loop over all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Process each CSV file\n",
    "            print(f\"Processing {filename}...\")\n",
    "            processed_data = process_chas_data(csv_path, tahoe_census_tracts, chas_variables)\n",
    "            \n",
    "            # Append to the list\n",
    "            all_data.append(processed_data)\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    final_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "directory_path = r'C:\\Users\\amcclary\\Downloads\\2017thru2021-140-csv\\140'  # Update with your directory path\n",
    "# Ensure tahoe_census_tracts and chas_variables are defined before calling\n",
    "# final_dataframe = process_all_csv_in_directory(directory_path, tahoe_census_tracts, chas_variables)\n",
    "\n",
    "# Display first few rows of the resulting combined dataframe\n",
    "# print(final_dataframe.head())\n",
    "\n",
    "all_hud_data = process_all_csv_in_directory(directory_path, tahoe_census_tracts, chas_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hud_data['TRPAID']= all_hud_data['geoid']+'2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hud_data.to_csv(r'C:\\Users\\amcclary\\Documents\\GitHub\\Housing\\Scripts\\Dowloaded_Data\\all_hud_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas_data_local_merged = chas_data_melted.merge(chas_variables, how='inner', on='variable_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data = summarize_census_data('Lookup_Lists/housing_employment_census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data.to_csv('census_employment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_household = pd.read_csv('Lookup_Lists/demographic_variables_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_demographics = pd.read_csv('Lookup_Lists/demographic_variables_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df_census to only include variables in the variables_demographics list joined on sample_year, sample_level and variable_name\n",
    "df_census_demographics = df_census.merge(variables_demographics, how='inner', left_on=['year_sample', 'sample_level', 'variable_name'], right_on=['year', 'geography', 'Variable Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group df_census_demographics by sample_year, tract, and variable_name and sum the values\n",
    "df_census_demographics_grouped = df_census_demographics.groupby(['year_sample', 'tract','state', \n",
    "\n",
    "                                                                 'county', 'variable_name', 'variable_code', 'census_geom_year_x'])['value'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census_demographics_grouped['year_sample'] = df_census_demographics_grouped['year_sample'].astype(int).astype(str)\n",
    "df_census_demographics_grouped['census_geom_year_x'] = df_census_demographics_grouped['census_geom_year_x'].astype(int).astype(str)\n",
    "df_census_demographics_grouped['trpa_id'] = df_census_demographics_grouped['state'] + df_census_demographics_grouped['county'] + df_census_demographics_grouped['tract']+df_census_demographics_grouped['census_geom_year_x']\n",
    "#Rename census_geom_year_x to census_geom_year\n",
    "df_census_demographics_grouped.rename(columns={'census_geom_year_x': 'census_geom_year'}, inplace=True)\n",
    "df_census_demographics_grouped.to_csv('Summarized_Data/Demographics_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment_data = pd.read_csv('Lookup_Lists/employment_2022_data.csv')\n",
    "tract_lookup = pd.read_csv('Lookup_Lists/TAZ_Tract_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214 entries, 0 to 213\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   TAZ                    214 non-null    int64  \n",
      " 1   ZIPCODE                209 non-null    float64\n",
      " 2   emp_other              213 non-null    float64\n",
      " 3   emp_rec                137 non-null    float64\n",
      " 4   STATE                  209 non-null    object \n",
      " 5   emp_retail             146 non-null    float64\n",
      " 6   emp_srvc               155 non-null    float64\n",
      " 7   emp_gaming             98 non-null     float64\n",
      " 8   adjustment_factor_CBP  209 non-null    float64\n",
      " 9   job_change_CBP         209 non-null    float64\n",
      " 10  2018                   116 non-null    float64\n",
      " 11  2022                   116 non-null    float64\n",
      " 12  adjustment_factor_EDD  116 non-null    float64\n",
      " 13  job_change_EDD         116 non-null    float64\n",
      " 14  emp_other_2022         210 non-null    float64\n",
      " 15  emp_rec_2022           134 non-null    float64\n",
      " 16  emp_retail_2022        143 non-null    float64\n",
      " 17  emp_srvc_2022          152 non-null    float64\n",
      " 18  emp_gaming_2022        4 non-null      float64\n",
      " 19  tract                  214 non-null    int64  \n",
      "dtypes: float64(17), int64(2), object(1)\n",
      "memory usage: 33.6+ KB\n"
     ]
    }
   ],
   "source": [
    "taz_employment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment_data['TRPAID'] = taz_employment_data.TAZ.map(dict(zip(tract_lookup.TAZ_Current, tract_lookup['TRPAID'])))\n",
    "taz_employment_data.fillna(0, inplace=True)\n",
    "tract_employment_data = taz_employment_data.groupby(['TRPAID'])[['emp_other_2022','emp_rec_2022',\n",
    "                                                     'emp_retail_2022','emp_srvc_2022','emp_gaming_2022']].sum().reset_index()\n",
    "tract_employment_data.to_csv('Employment_Data_EDD_DETR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some data cleanup on this and fill in blanks. What is happening with group quarters???\n",
    "\n",
    "# Add group quarters by type to the data\n",
    "#change geoid type to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = sdeCollect + \"\\\\SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_variable_list = pd.read_csv('Lookup_Lists/census_variables.csv')\n",
    "df_census_2022_include = df_census_2022.loc[df_census_2022['variable_code'].isin(census_variable_list['variable_code'])]\n",
    "block_group_pivot = df_census_2022_include.pivot(index='block_group', columns='variable_code', values='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_attributed = pd.merge(sdf_units, block_group_pivot, left_on='TRPAID', right_on='TRPAID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcel level summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in parcel history attributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes two columns in a dataframe and divides one by the other and assigns that v\n",
    "def divide_columns(df, col1, col2, new_col):\n",
    "    df[new_col] = df[col1] / df[col2]\n",
    "    return df\n",
    "# take that function and ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def divide_columns_from_csv(df, csv_path, variable_column):\n",
    "    # Load the CSV file with column specifications\n",
    "    variable_list = pd.read_csv(csv_path)\n",
    "\n",
    "    # Iterate through each row in the CSV\n",
    "    for _, row in variable_list.iterrows():\n",
    "        col1 = row['col1']\n",
    "        col2 = row['col2']\n",
    "        col3 = row['col3']\n",
    "        new_col = row['new_col']\n",
    "\n",
    "        # Select values from the dataframe where the variable column matches col1 and col2\n",
    "        value1 = df.loc[df[variable_column] == col1, 'value'].values[0]\n",
    "        value2 = df.loc[df[variable_column] == col2, 'value'].values[0]\n",
    "\n",
    "        # Perform division and handle potential division by zero\n",
    "        result = value1 / value2 if value2 != 0 else float('nan')\n",
    "        \n",
    "        # Add the result to the dataframe\n",
    "        df.loc[df[variable_column] == col1, new_col] = result\n",
    "        df.loc[df[variable_column]==col1, 'variable_name'] = col3\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed approach to attributing the tract level dataframe with a proportion. Should be cleaner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
