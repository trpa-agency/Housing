{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import arcpy\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "# external connection packages\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pandas options\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows    = 999\n",
    "\n",
    "# my workspace \n",
    "workspace = r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "# current working directory\n",
    "local_path = pathlib.Path().absolute()\n",
    "# set data path as a subfolder of the current working directory TravelDemandModel\\2022\\\n",
    "data_dir = local_path.parents[0] / 'data'\n",
    "# folder to save processed data\n",
    "out_dir  = local_path.parents[0] / 'data/processed_data'\n",
    "# workspace gdb for stuff that doesnt work in memory\n",
    "# gdb = os.path.join(local_path,'Workspace.gdb')\n",
    "gdb = workspace\n",
    "# set environement workspace to in memory \n",
    "arcpy.env.workspace = 'memory'\n",
    "# # clear memory workspace\n",
    "# arcpy.management.Delete('memory')\n",
    "\n",
    "# overwrite true\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "# get parcels from the database\n",
    "# network path to connection files\n",
    "filePath = \"F:/GIS/PARCELUPDATE/Workspace/\"\n",
    "# database file path \n",
    "sdeBase    = os.path.join(filePath, \"Vector.sde\")\n",
    "sdeCollect = os.path.join(filePath, \"Collection.sde\")\n",
    "sdeTabular = os.path.join(filePath, \"Tabular.sde\")\n",
    "sdeEdit    = os.path.join(filePath, \"Edit.sde\")\n",
    "\n",
    "# Pickle variables\n",
    "# part 1 - spatial joins and new categorical fields\n",
    "parcel_pickle_part1    = data_dir / 'parcel_pickle1.pkl'\n",
    "# part 2 - forecasting applied\n",
    "parcel_pickle_part2    = data_dir / 'parcel_pickle2.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Fields to Keep\n",
    "\n",
    "# columsn to list\n",
    "initial_columns = [ 'APN',\n",
    "                    'APO_ADDRESS',\n",
    "                    'Residential_Units',\n",
    "                    'TouristAccommodation_Units',\n",
    "                    'CommercialFloorArea_SqFt',\n",
    "                    'YEAR',\n",
    "                    'JURISDICTION',\n",
    "                    'COUNTY',\n",
    "                    'OWNERSHIP_TYPE',\n",
    "                    'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                    'EXISTING_LANDUSE',\n",
    "                    'REGIONAL_LANDUSE',\n",
    "                    'YEAR_BUILT',\n",
    "                    'PLAN_ID',\n",
    "                    'PLAN_NAME',\n",
    "                    'ZONING_ID',\n",
    "                    'ZONING_DESCRIPTION',\n",
    "                    'TOWN_CENTER',\n",
    "                    'LOCATION_TO_TOWNCENTER',\n",
    "                    'TAZ',\n",
    "                    'PARCEL_ACRES',\n",
    "                    'PARCEL_SQFT',\n",
    "                    'WITHIN_BONUSUNIT_BNDY',\n",
    "                    'WITHIN_TRPA_BNDY',\n",
    "                    'SHAPE']\n",
    "\n",
    "# parcel master fields\n",
    "pm_fields = ['AS_LANDVALUE',\n",
    "                'AS_IMPROVALUE',\n",
    "                'AS_SUM',\n",
    "                'TAX_LANDVALUE',\n",
    "                'TAX_IMPROVALUE',\n",
    "                'TAX_SUM',\n",
    "                'TAX_YEAR',\n",
    "                'YEAR_BUILT',\n",
    "                'UNITS',\n",
    "                'BEDROOMS',\n",
    "                'BATHROOMS',\n",
    "                'BUILDING_SQFT',\n",
    "                'ESTIMATED_COVERAGE_ALLOWED',\n",
    "                'IMPERVIOUS_SURFACE_SQFT']\n",
    "\n",
    "\n",
    "# schema for the final output\n",
    "final_schema = ['APN', \n",
    "                \n",
    "                'Residential_Units', \n",
    "                'TouristAccommodation_Units', \n",
    "                'CommercialFloorArea_SqFt',\n",
    "                \n",
    "                'RoomsRented_PerDay', \n",
    "                'VHR_Occupancy_Rate',\n",
    "                'TAU_Occupancy_Rate', \n",
    "                'PrimaryResidence_Rate', \n",
    "                'SecondaryResidence_Rate',\n",
    "                'HighIncome_Rate',\t\n",
    "                'MediumIncome_Rate', \n",
    "                'LowIncome_Rate', \n",
    "                'PersonsPerUnit',\n",
    "\n",
    "                'APO_ADDRESS',\n",
    "                'YEAR_BUILT',\n",
    "                'JURISDICTION', \n",
    "                'COUNTY', \n",
    "                'OWNERSHIP_TYPE',\n",
    "                'COUNTY_LANDUSE_DESCRIPTION',\n",
    "                'EXISTING_LANDUSE',\n",
    "                'REGIONAL_LANDUSE',\n",
    "                'VHR', \n",
    "                'BLOCK_GROUP', \n",
    "                'TAZ', \n",
    "                'OCCUPANCY_ZONE', \n",
    "                'PLAN_ID',\n",
    "                'PLAN_NAME',\n",
    "                'ZONING_ID',\n",
    "                'ZONING_DESCRIPTION',\n",
    "                'TOWN_CENTER',\n",
    "                'LOCATION_TO_TOWNCENTER',\n",
    "                'TAZ',\n",
    "                'WITHIN_BONUSUNIT_BNDY',\n",
    "                'WITHIN_TRPA_BNDY',\n",
    "                'IPES_SCORE',\n",
    "                'IPES_SCORE_TYPE',\n",
    "                'RETIRED',\n",
    "                'HOUSING_ZONING',\n",
    "                'COMMERCIAL_ALLOWED',\n",
    "                'TOURIST_ALLOWED',\n",
    "                'ADU_ALLOWED',\n",
    "                'DENSITY',\n",
    "                'MAX_RESIDENTIAL_UNITS',\n",
    "                'MAX_UNITS',\n",
    "                'POTENTIAL_UNITS',\n",
    "                'TOP_TEN_POTENTIAL_UNITS',\n",
    "                'FORECASTED_RESIDENTIAL_UNITS',\n",
    "                'FORECASTED_COMMERCIAL_SQFT',\n",
    "                'FORECASTED_TOURIST_UNITS',\n",
    "                'FORECAST_REASON',\n",
    "                'FORECASTED_RES_OCCUPANCY_RATE',    \n",
    "                'PARCEL_ACRES', 'PARCEL_SQFT', 'SHAPE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql server connection\n",
    "def get_conn(db):\n",
    "    # Get database user and password from environment variables on machine running script\n",
    "    db_user             = os.environ.get('DB_USER')\n",
    "    db_password         = os.environ.get('DB_PASSWORD')\n",
    "    # driver is the ODBC driver for SQL Server\n",
    "    driver              = 'ODBC Driver 17 for SQL Server'\n",
    "    # server names are\n",
    "    sql_12              = 'sql12'\n",
    "    sql_14              = 'sql14'\n",
    "    # make it case insensitive\n",
    "    db = db.lower()\n",
    "    # make sql database connection with pyodbc\n",
    "    if db   == 'sde_tabular':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'tahoebmpsde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_14};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    elif db == 'sde':\n",
    "        connection_string = f\"DRIVER={driver};SERVER={sql_12};DATABASE={db};UID={db_user};PWD={db_password}\"\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        engine = create_engine(connection_url)\n",
    "    # else return None\n",
    "    else:\n",
    "        engine = None\n",
    "    # connection file to use in pd.read_sql\n",
    "    return engine\n",
    "\n",
    "# save to pickle\n",
    "def to_pickle(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled')\n",
    "\n",
    "# save to pickle and feature class\n",
    "def to_pickle_fc(data, filename):\n",
    "    data.spatial.to_featureclass(filename)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f'{filename} pickled and saved as feature class')\n",
    "\n",
    "# get a pickled file as a dataframe\n",
    "def from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'{filename} unpickled')\n",
    "    return data\n",
    "def get_commercial_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Category', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Category'] == 'Commercial']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_tourist_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Category', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Category'] == 'Tourist Accommodation']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Multiple Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "# function to get where Zoningin_ID Use_Type = Multi-Family and Density\n",
    "def get_sf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Multiple Family Dwelling\n",
    "    df = df.loc[df['Use_Type'] == 'Single Family Dwelling']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_mf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    # get Zoning_ID that are in both dataframes\n",
    "    df = dfMF.loc[~dfMF['Zoning_ID'].isin(dfSF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_only_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # filter Use_Type to Single Family Dwelling and not Multiple Family Dwelling\n",
    "    dfMF = get_mf_zones(df)\n",
    "    dfSF = get_sf_zones(df)\n",
    "    df = dfSF.loc[~dfSF['Zoning_ID'].isin(dfMF['Zoning_ID'])]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sf_mf_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'Use_Type', 'Density']\n",
    "    # get SF and MF zones\n",
    "    dfSF = get_sf_zones(df)\n",
    "    dfMF = get_mf_zones(df)\n",
    "    # add the two dataframes together\n",
    "    df = pd.concat([dfSF, dfMF])\n",
    "    # only keep duplicate Zoning_ID\n",
    "    df = df[df.duplicated(subset=['Zoning_ID'], keep=False)]\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_recieving_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    # filter transfer recieving\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Receive']\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def get_sending_zones(df):\n",
    "    columns_to_keep = ['Zoning_ID', 'SPECIAL_DESIGNATION']\n",
    "    df = df.loc[df['SPECIAL_DESIGNATION'] == 'Transfer']\n",
    "    return df[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parcel master\n",
    "parcelmaster_db = Path(sdeBase) / \"SDE.Parcels\\SDE.Parcel_Master\"\n",
    "sdf_master = pd.DataFrame.spatial.from_featureclass(parcelmaster_db)\n",
    "sdf_master.spatial.sr = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_master.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development layer polygons\n",
    "parcel_db = Path(sdeEdit) / \"SDE.Parcel\\\\SDE.Parcel_History_Attributed\"\n",
    "# query 2022 rows\n",
    "sdf_units = pd.DataFrame.spatial.from_featureclass(parcel_db)\n",
    "sdf_units = sdf_units.loc[sdf_units['YEAR'] == 2022]\n",
    "sdf_units.spatial.sr = sr\n",
    "\n",
    "# get parcel master\n",
    "parcelmaster_db = Path(sdeBase) / \"SDE.Parcel\\\\SDE.Parcel_Master\"\n",
    "sdf_master = pd.DataFrame.spatial.from_featureclass(parcelmaster_db)\n",
    "sdf_master.spatial.sr = sr\n",
    "\n",
    "# # get parcel level data from Collection SDE\n",
    "# vhr feature layer polygons \n",
    "vhr_db = Path(sdeCollect) / \"SDE.Parcel\\\\SDE.Parcel_VHR\"\n",
    "sdf_vhr = pd.DataFrame.spatial.from_featureclass(vhr_db)\n",
    "sdf_vhr.spatial.sr = sr\n",
    "# filter vhr layer to active status\n",
    "sdf_vhr = sdf_vhr.loc[sdf_vhr['Status'] == 'Active']\n",
    "\n",
    "# TAZ feature layer polygons\n",
    "taz_db = Path(sdeBase) / \"SDE.Transportation\\\\SDE.Transportation_Analysis_Zone\"\n",
    "# get as spatial dataframe\n",
    "sdf_taz = pd.DataFrame.spatial.from_featureclass(taz_db)\n",
    "# set spatial reference to NAD 1983 UTM Zone 10N\n",
    "sdf_taz.spatial.sr = sr\n",
    "\n",
    "# censuse feature class\n",
    "census_fc    = Path(sdeBase) / \"SDE.Census\\\\SDE.Tahoe_Census_Geography\"\n",
    "# bouns unit boundary feature class\n",
    "bonus_unit_fc = Path(sdeBase) / \"SDE.Planning\\SDE.Bonus_unit_boundary\"\n",
    "\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Census\\SDE.Tahoe_Census_Geography\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "# disable Z values on block group feature layer\n",
    "with arcpy.EnvManager(outputZFlag=\"Disabled\"):    \n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features=\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Planning\\SDE.Bonus_unit_boundary\",\n",
    "        Output_Geodatabase=r\"C:\\Users\\mbindl\\Desktop\\Workspace.gdb\"\n",
    "    )\n",
    "\n",
    "# block group feature layer polygons with no Z\n",
    "sdf_block = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Tahoe_Census_Geography')\n",
    "sdf_block = sdf_block.loc[(sdf_block['YEAR'] == 2020) & (sdf_block['GEOGRAPHY'] == 'Block Group')]\n",
    "sdf_block.spatial.sr = sr\n",
    "\n",
    "# bonus unit boundary wihtout Z\n",
    "sdf_bonus = pd.DataFrame.spatial.from_featureclass(Path(gdb) / 'Bonus_unit_boundary')\n",
    "sdf_bonus.spatial.sr = sr\n",
    "\n",
    "# get parcel level data from LTinfo\n",
    "dfIPES       = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetParcelIPESScores/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfLCV_LTinfo = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetParcelsByLandCapability/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfRetired    = pd.read_json(\"https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\")\n",
    "dfBankedDev  = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfTransacted = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetTransactedAndBankedDevelopmentRights/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "dfAllParcels = pd.read_json('https://www.laketahoeinfo.org/WebServices/GetAllParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476')\n",
    "\n",
    "# get use tables \n",
    "# zoning data\n",
    "sde_engine = get_conn('sde')\n",
    "with sde_engine.begin() as conn:\n",
    "    df_uses    = pd.read_sql(\"SELECT * FROM sde.SDE.PermissibleUses\", conn)\n",
    "    df_special = pd.read_sql(\"SELECT * FROM sde.SDE.Special_Designation\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parcel Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to get TAZ\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_taz, \"Existing_Development_TAZ\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join to get Block Group\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_block, \"Existing_Development_BlockGroup\", \n",
    "                           \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"HAVE_THEIR_CENTER_IN\")\n",
    "# spatial join of Bonus Unit Boundary\n",
    "arcpy.SpatialJoin_analysis(sdf_units, sdf_bonus, \"Existing_Development_BonusUnitBoundary\",\n",
    "                            \"JOIN_ONE_TO_ONE\", \"KEEP_ALL\", \"\", \"INTERSECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial dataframe with only initial columns\n",
    "sdfParcels = sdf_units[initial_columns]\n",
    "\n",
    "# get results of spatial joins as spatial dataframes\n",
    "sdf_units_taz   = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_TAZ\", sr=sr)  \n",
    "sdf_units_block = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BlockGroup\", sr=sr)\n",
    "sdf_units_bonus = pd.DataFrame.spatial.from_featureclass(\"Existing_Development_BonusUnitBoundary\", sr=sr)\n",
    "# cast to string\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'].astype(str)\n",
    "sdf_units_bonus['WITHIN_BONUSUNIT_BNDY'] = 'No'\n",
    "# if Id is not NA then within bonus unit boundary = yes, else\n",
    "sdf_units_bonus.loc[sdf_units_bonus['Id'].notna(), 'WITHIN_BONUSUNIT_BNDY'] = 'Yes'\n",
    "\n",
    "# map dictionary to sdf_units dataframe to fill in TAZ and Block Group fields\n",
    "sdfParcels['TAZ']                   = sdfParcels.APN.map(dict(zip(sdf_units_taz.APN,   sdf_units_taz.TAZ_1)))\n",
    "sdfParcels['BLOCK_GROUP']           = sdfParcels.APN.map(dict(zip(sdf_units_block.APN, sdf_units_block.TRPAID)))\n",
    "# map IPES score to parcels\n",
    "sdfParcels['IPES_SCORE']            = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScore)))\n",
    "sdfParcels['IPES_SCORE_TYPE']       = sdfParcels['APN'].map(dict(zip(dfIPES.APN, dfIPES.IPESScoreType)))\n",
    "# retired parcels\n",
    "sdfParcels['RETIRED']               = sdfParcels['APN'].map(dict(zip(dfAllParcels.APN, dfAllParcels.RetiredFromDevelopment)))\n",
    "sdfParcels['WITHIN_BONUSUNIT_BNDY'] = sdfParcels['APN'].map(dict(zip(sdf_units_bonus.APN, sdf_units_bonus.WITHIN_BONUSUNIT_BNDY)))\n",
    "# define housnig zoning and density\n",
    "sdfParcels['HOUSING_ZONING']          = 'NA'\n",
    "sdfParcels['COMMERCIAL_ALLOWED']      = 'No'\n",
    "sdfParcels['TOURIST_ALLOWED']         = 'No'\n",
    "\n",
    "# if the zoning id is in the list of multiple family zones then set to MF\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_mf_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF/MF'\n",
    "# if the zoning id is in the list of single family zones and not in the multiple family zones then set to SF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_sf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'SF_only'\n",
    "# if the zoning id is in the list of multiple family zones and not in the single family zones then set to MF only\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_mf_only_zones(df_uses)['Zoning_ID']), 'HOUSING_ZONING'] = 'MF_only'\n",
    "# if the zoning id is in the list of commercial zones then set to Commercial\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_commercial_zones(df_uses)['Zoning_ID']), 'COMMERCIAL_ALLOWED'] = 'Yes'\n",
    "# if the zoning id is in the list of tourist zones then set to Tourist Accommodation\n",
    "sdfParcels.loc[sdfParcels['ZONING_ID'].isin(get_tourist_zones(df_uses)['Zoning_ID']), 'TOURIST_ALLOWED'] = 'Yes'\n",
    "\n",
    "# if COUNTY is in EL or PL and SF allowed then set ADU_ALLOWED to yes or if COUNTY is in WA, DG, or CC and parcel acres is greater than 1 and SF allowed then set ADU_ALLOWED to yes\n",
    "sdfParcels['ADU_ALLOWED'] = 'No'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['EL','PL'])) & (~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "sdfParcels.loc[(sdfParcels['COUNTY'].isin(['WA','DG','CC'])) & (sdfParcels['PARCEL_ACRES']>=1) &(~sdfParcels['HOUSING_ZONING'].isin(['MF_only', 'NA'])), 'ADU_ALLOWED'] = 'Yes'\n",
    "\n",
    "# get density for MF and MF only zones, max residential units, and adjusted residential units\n",
    "dfMF = get_mf_zones(df_uses)\n",
    "sdfParcels['DENSITY']                    = sdfParcels['ZONING_ID'].map(dict(zip(dfMF.Zoning_ID, dfMF.Density)))\n",
    "sdfParcels['MAX_RESIDENTIAL_UNITS']      = sdfParcels['PARCEL_ACRES'] * sdfParcels['DENSITY']\n",
    "sdfParcels['MAX_UNITS']                  = sdfParcels['MAX_RESIDENTIAL_UNITS']*0.6\n",
    "sdfParcels['MAX_UNITS']                  = sdfParcels['MAX_UNITS'].fillna(0).astype(int)\n",
    "\n",
    "# set SF only zones to 1 max unit\n",
    "sdfParcels.loc[sdfParcels['HOUSING_ZONING'] == 'SF_only', 'MAX_UNITS'] = 1\n",
    "\n",
    "# set field for underbuilt evaluation\n",
    "sdfParcels['POTENTIAL_UNITS'] = 0\n",
    "sdfParcels['POTENTIAL_UNITS'] = sdfParcels['MAX_UNITS'] - sdfParcels['Residential_Units']\n",
    "# set negative values to 0\n",
    "sdfParcels.loc[sdfParcels['POTENTIAL_UNITS'] < 0, 'POTENTIAL_UNITS'] = 0\n",
    "\n",
    "# calculate parcels with the greatest buildable potential  filter to the top 10% of parcels\n",
    "# what value is in the top 10% of the potential buildable units\n",
    "top_10_threshold = sdfParcels.POTENTIAL_UNITS.quantile(0.9)\n",
    "# filter out rows where POTENTIAL_BUILDABLE_UNITS is NaN\n",
    "sdfParcels['TOP_TEN_POTENTIAL_UNITS'] = sdfParcels.apply(lambda x: 'Yes' if x['POTENTIAL_UNITS'] >= top_10_threshold else 'No', axis=1)\n",
    "\n",
    "# set FORECASTED_RESIDENTIAL_UNITS to 0\n",
    "sdfParcels['FORECASTED_RESIDENTIAL_UNITS']     = 0\n",
    "# set FORECAST_COMMERCIAN_UNITS to 0\n",
    "sdfParcels['FORECASTED_COMMERCIAL_SQFT']       = 0\n",
    "# set FORECAST_TOURIST_UNITS to 0\n",
    "sdfParcels['FORECASTED_TOURIST_UNITS']         = 0\n",
    "# set FORECAST_REASON to na\n",
    "sdfParcels['FORECAST_REASON']                  = None\n",
    "# FORECASTED_OCCUPANCY_RATE as a float field\n",
    "sdfParcels['FORECASTED_RES_OCCUPANCY_RATE']    = 0.0\n",
    "\n",
    "# export to pickle\n",
    "sdfParcels.to_pickle(parcel_pickle_part1)\n",
    "# to feature class\n",
    "sdfParcels.spatial.to_featureclass(Path(gdb)/'Parcel_Base_2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant census variables and calculate rates at block group level\n",
    "# Get Occupancy Data - B25002_003E = Vacant, B25002_002E = Occupied , B25004_006E = Vacant Seasonal\n",
    "occupancy_codes = ['B25002_003E','B25002_002E', 'B25004_006E']\n",
    "df_census_occupancy = df_census_2022[df_census_2022['variable_code'].isin(occupancy_codes)]\n",
    "df_census_occupancy = df_census_occupancy[['TRPAID', 'variable_code', 'value']]\n",
    "# pivot to wide format so we can calculate percentages and totals\n",
    "df_census_occupancy = df_census_occupancy.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "# vacant units + occupied units = total units\n",
    "df_census_occupancy['total_units'] = df_census_occupancy['B25002_003E'] + df_census_occupancy['B25002_002E']\n",
    "# occupancy rate = occupied units / total units\n",
    "df_census_occupancy['occupancy_rate'] = df_census_occupancy['B25002_002E'] / df_census_occupancy['total_units']\n",
    "# seasonal rate = seasonal units / total units\n",
    "df_census_occupancy['seasonal_rate'] = df_census_occupancy['B25004_006E'] / df_census_occupancy['total_units']\n",
    "\n",
    "\n",
    "# Get Household Size Data - B25010_001E = Total Households\n",
    "df_census_household_size = df_census_2022[df_census_2022['variable_code'] == 'B25010_001E']\n",
    "df_census_household_size = df_census_household_size[['TRPAID', 'variable_code', 'value']]\n",
    "df_census_household_size = df_census_household_size.pivot(index='TRPAID', columns='variable_code', values='value').reset_index()\n",
    "df_census_household_size['household_size'] = df_census_household_size['B25010_001E']\n",
    "#adjust household size by a constant factor so that total residents = total population from acs\n",
    "ACS_total_population = 53842\n",
    "# This is from a final calculated input summary - will eventually make an explicit calculation\n",
    "total_calculated_population = 53019.75\n",
    "household_size_adjustment = ACS_total_population / total_calculated_population\n",
    "df_census_household_size['household_size_raw'] = df_census_household_size['household_size']\n",
    "df_census_household_size['household_size'] = df_census_household_size['household_size'] * household_size_adjustment\n",
    "\n",
    "\n",
    "# List of Codes by the category they fall into - Census categroy to broader category\n",
    "code_lookup = pd.read_csv('Lookup_Lists/income_census_codes.csv')\n",
    "#Filter census so only variable codes in the code lookup are included\n",
    "df_census_income = df_census_2022[df_census_2022['variable_code'].isin(code_lookup['variable_code'])]\n",
    "#Create a new column that has a value from code lookup based on the variable code\n",
    "df_census_income['income_category'] = df_census_income['variable_code'].map(code_lookup.set_index('variable_code')['category'])\n",
    "#group by block group and income category and sum the values\n",
    "df_census_income = df_census_income.groupby(['TRPAID','income_category'])['value'].sum().reset_index()\n",
    "df_census_income = df_census_income.pivot(index='TRPAID', columns='income_category', values='value').reset_index()\n",
    "\n",
    "\n",
    "# TRPAID is a 16 digit ID, but it is imported as a float. Convert to string and to retain leading zeros\n",
    "df_census_household_size['TRPAID']= df_census_household_size['TRPAID'].astype(str).str.zfill(16)\n",
    "df_census_income['TRPAID']= df_census_income['TRPAID'].astype(str).str.zfill(16)\n",
    "# merge all the census data together\n",
    "df_census_occupancy_all = pd.merge(df_census_occupancy, df_census_household_size, on='TRPAID', how='left')\n",
    "df_census_all = pd.merge(df_census_occupancy_all, df_census_income, on='TRPAID', how='left')\n",
    "# rename columns of df_census_all\n",
    "column_rename = {\n",
    "    'B25002_003E': 'vacant_units',\n",
    "    'B25002_002E': 'occupied_units',\n",
    "    'B25004_006E': 'seasonal_units',\n",
    "    'High Income': 'high_income',\n",
    "    'Low Income': 'low_income',\n",
    "    'Medium Income': 'middle_income',\n",
    "}\n",
    "df_census_all.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "df_census_all.drop(columns=['B25010_001E'], inplace=True)\n",
    "# calculate proportions of income categories\n",
    "df_census_all['high_income_proportion'] = df_census_all['high_income'] / df_census_all['occupied_units']\n",
    "df_census_all['middle_income_proportion'] = df_census_all['middle_income'] / df_census_all['occupied_units']\n",
    "df_census_all['low_income_proportion'] = df_census_all['low_income'] / df_census_all['occupied_units']\n",
    "\n",
    "\n",
    "# get pickle part 3\n",
    "sdfParcel = pd.read_pickle(parcel_pickle_part3)\n",
    "\n",
    "# map values from Census data to parcel data via left_on BLOCK_GROUP and right_on TRPAID\n",
    "sdfParcel.PrimaryResidence_Rate   = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['occupancy_rate'])\n",
    "sdfParcel.SecondaryResidence_Rate = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['seasonal_rate'])\n",
    "sdfParcel.HighIncome_Rate         = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['high_income_proportion'])\n",
    "sdfParcel.MediumIncome_Rate       = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['middle_income_proportion'])\n",
    "sdfParcel.LowIncome_Rate          = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['low_income_proportion'])\n",
    "sdfParcel.PersonsPerUnit          = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['household_size'])\n",
    "sdfParcel['PersonsPerUnit_Raw']      = sdfParcel.BLOCK_GROUP.map(df_census_all.set_index('TRPAID')['household_size_raw'])\n",
    "\n",
    "# seasonal rate calculation\n",
    "# group by BLOCK_GROUP\n",
    "# filter sdfParcel where VHR   = 'Yes'\n",
    "vhrs = sdfParcel.loc[sdfParcel['VHR']=='Yes']\n",
    "totalRes = sdfParcel.groupby('BLOCK_GROUP').agg({'Residential_Units':'sum', 'PrimaryResidence_Rate':'mean', \n",
    "                                                 'SecondaryResidence_Rate':'mean'}).reset_index()\n",
    "totalVHR = vhrs.groupby('BLOCK_GROUP').agg({'Residential_Units':'sum'}).reset_index()\n",
    "totalVHR.rename(columns={'Residential_Units':'VHR_Units'}, inplace=True)\n",
    "\n",
    "# merge totalRes and totalVHR\n",
    "totalResVHR = pd.merge(totalRes, totalVHR, on='BLOCK_GROUP', how='left')\n",
    "# fill NA with 0\n",
    "totalResVHR.VHR_Units = totalResVHR.VHR_Units.fillna(0)\n",
    "# calculate seasonal rate\n",
    "totalResVHR['non_vhr_units'] = totalResVHR['Residential_Units'] - totalResVHR['VHR_Units']\n",
    "# calculate the non-adjusted number of seasonal units and then subtract the number of VHRs\n",
    "totalResVHR['non_adjusted_seasonal_units'] = totalResVHR['SecondaryResidence_Rate'] * totalResVHR['Residential_Units']\n",
    "totalResVHR['non_primary_residence_units'] = totalResVHR['Residential_Units']-(totalResVHR['PrimaryResidence_Rate'] * totalResVHR['Residential_Units'])\n",
    "totalResVHR['adjusted_seasonal_units']     = totalResVHR['non_adjusted_seasonal_units'] - totalResVHR['VHR_Units']\n",
    "# Manually adjust the seasonal units for block group 3200500170022020 because of a lag in the data\n",
    "# The census reports 100% occupancy but I think it has to do with the beach club development\n",
    "totalResVHR.loc[totalResVHR['BLOCK_GROUP'] == '3200500170022020', 'adjusted_seasonal_units'] = 0\n",
    "# calculate the adjusted seasonal rate\n",
    "totalResVHR['adjusted_seasonal_rate'] = totalResVHR['adjusted_seasonal_units'] / totalResVHR['non_primary_residence_units']\n",
    "\n",
    "# map the adjusted seasonal rate to the parcel data\n",
    "sdfParcel['SecondaryResidence_Rate'] = sdfParcel['BLOCK_GROUP'].map(totalResVHR.set_index('BLOCK_GROUP')['adjusted_seasonal_rate'])\n",
    "\n",
    "# export to pickle part 4\n",
    "sdfParcel.to_pickle(parcel_pickle_part2)\n",
    "sdfParcel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
